{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c349a330-d2dc-4d71-bbe9-6744dc246f9b",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Answer(Q1):\n",
    "\n",
    "Let's break down the information given in the problem:\n",
    "\n",
    "1. The probability that an employee uses the company's health insurance plan is 70%.\n",
    "2. Among the employees who use the health insurance plan, 40% are smokers.\n",
    "\n",
    "We are asked to find the probability that an employee is a smoker given that they use the health insurance plan. This can be calculated using conditional probability.\n",
    "\n",
    "Let's define the events:\n",
    "A: Employee is a smoker.\n",
    "B: Employee uses the health insurance plan.\n",
    "\n",
    "We are looking for P(A|B), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "The formula for conditional probability is:\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "In this case:\n",
    "P(A and B) = P(B) * P(A|B) = 0.70 * 0.40 = 0.28\n",
    "\n",
    "P(B) = 0.70 (since 70% of employees use the health insurance plan)\n",
    "\n",
    "Now, we can calculate P(A|B):\n",
    "P(A|B) = P(A and B) / P(B) = 0.28 / 0.70 = 0.4\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.4, or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba435d52-76a8-4158-875f-e961c6999cda",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "\n",
    "Answer(Q2):\n",
    "\n",
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variations of the Naive Bayes algorithm, which is commonly used for classification tasks, especially in text classification and natural language processing.\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are best suited for and the assumptions they make about the data.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Data Type:** Bernoulli Naive Bayes is used when dealing with binary or boolean features. It's suitable when you have data that represents the presence or absence of certain features. For example, in text classification, you might represent the presence (1) or absence (0) of specific words in a document.\n",
    "   - **Assumptions:** It assumes that each feature is independent of others given the class label, and it models the presence or absence of a feature in a document. It's called \"Bernoulli\" because it models data using the Bernoulli distribution, which is a discrete probability distribution for a random variable that can take on one of two possible outcomes (usually represented as 0 or 1).\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Data Type:** Multinomial Naive Bayes is suitable for data with discrete features that represent counts or frequencies, such as word counts in text. It's commonly used for text classification tasks where you have a set of words and their frequencies in a document.\n",
    "   - **Assumptions:** Like other Naive Bayes variants, it assumes that the features are conditionally independent given the class label. It models the distribution of word frequencies within each class using the multinomial distribution, which is a generalization of the binomial distribution to multiple categories.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used when dealing with binary or boolean features (presence/absence), and it models data using the Bernoulli distribution. Multinomial Naive Bayes is used when working with discrete features that represent counts or frequencies, often in text classification scenarios, and it models data using the multinomial distribution. The choice between the two depends on the nature of your data and the specific problem you're trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef44e2d-295e-42e3-8c0e-eac23e1390ff",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "\n",
    "Answer(Q3):\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values by treating them as a separate category or by ignoring them during probability calculations, depending on the specific implementation or approach used. However, how missing values are handled can vary based on the context and the specific implementation you are using.\n",
    "\n",
    "Here are a couple of common approaches:\n",
    "\n",
    "1. **Treating Missing Values as a Separate Category:**\n",
    "   In Bernoulli Naive Bayes, features are typically binary or boolean, representing the presence (1) or absence (0) of a particular attribute. When dealing with missing values, you can consider introducing a separate category to represent missing values. This way, the feature becomes a ternary variable: 1 for presence, 0 for absence, and another value (let's say 2) to indicate a missing value.\n",
    "\n",
    "2. **Ignoring Missing Values:**\n",
    "   Another approach is to simply ignore the instances with missing values when calculating probabilities. In this case, you would exclude instances with missing values from both the numerator and the denominator when computing the probabilities for the Naive Bayes formula.\n",
    "\n",
    "The choice between these approaches depends on the nature of the problem and the characteristics of the dataset. Introducing a separate category for missing values might work well if missingness itself is meaningful information. On the other hand, ignoring missing values might be appropriate if the missingness is random or if including a separate category doesn't make sense for your specific application.\n",
    "\n",
    "Keep in mind that the effectiveness of these approaches can depend on the amount of missing data, the underlying distribution of the data, and the implications of handling missing values in your particular problem domain. Additionally, the way missing values are handled might be influenced by the implementation or library you're using to perform Bernoulli Naive Bayes. Always consider the context and characteristics of your data when deciding how to handle missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aa5b7-fbc4-4a9c-9595-6d9f9d1ea8aa",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "\n",
    "Answer(Q4):\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution within each class. It's commonly used for continuous or numerical features.\n",
    "\n",
    "When it comes to multi-class classification, where there are more than two classes to predict, Gaussian Naive Bayes can still be applied. The algorithm extends naturally to handle multiple classes by calculating the class probabilities and feature likelihoods for each class and then selecting the class with the highest posterior probability as the predicted class.\n",
    "\n",
    "Here's a brief overview of how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "1. **Training Phase:**\n",
    "   - For each class, calculate the mean and standard deviation of each feature. These parameters represent the Gaussian distribution of the features within each class.\n",
    "   - Estimate the prior probabilities for each class based on the proportions of training samples in each class.\n",
    "\n",
    "2. **Prediction Phase:**\n",
    "   - Given a new data point with feature values, calculate the likelihood of those feature values occurring in each class using the Gaussian probability density function.\n",
    "   - Multiply the prior probability of each class with the likelihood of the feature values for that class.\n",
    "   - Normalize the calculated probabilities for each class so that they sum up to 1.\n",
    "   - Select the class with the highest normalized probability as the predicted class.\n",
    "\n",
    "It's important to note that Gaussian Naive Bayes assumes that the features within each class follow a Gaussian distribution. If this assumption is reasonable for your data, Gaussian Naive Bayes can perform well. However, if your data doesn't meet this assumption, the performance of Gaussian Naive Bayes might be suboptimal.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can indeed be used for multi-class classification, making it a versatile algorithm for a variety of classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c85c93-360d-4bb2-9e17-60443e474a43",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/ datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    " Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51aa1b10-fe24-488d-b71b-8c28dfe3744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bernoulli Naive Bayes:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.89\n",
      "Recall: 0.82\n",
      "F1 Score: 0.85\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.74\n",
      "Recall: 0.72\n",
      "F1 Score: 0.73\n",
      "\n",
      "Results for Gaussian Naive Bayes:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.71\n",
      "Recall: 0.96\n",
      "F1 Score: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f'feature_{i}' for i in range(57)] + [\"target\"]\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_classifier = BernoulliNB()\n",
    "multinomial_classifier = MultinomialNB()\n",
    "gaussian_classifier = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate metrics\n",
    "def evaluate_classifier(classifier, name):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring=\"accuracy\").mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring=\"precision\").mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring=\"recall\").mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring=\"f1\").mean()\n",
    "    \n",
    "    print(f\"Results for {name} Naive Bayes:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_classifier(bernoulli_classifier, \"Bernoulli\")\n",
    "evaluate_classifier(multinomial_classifier, \"Multinomial\")\n",
    "evaluate_classifier(gaussian_classifier, \"Gaussian\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878d398-f418-4a77-b5ba-70eaf7b73812",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "Based on the results, you can analyze which variant of Naive Bayes performed the best in terms of accuracy, precision, recall, and F1 score. Compare the metrics for each classifier and consider their strengths and limitations:\n",
    "\n",
    "Bernoulli Naive Bayes: Performs well for binary features (presence/absence). It may perform well if the data's binary nature suits the assumptions.\n",
    "\n",
    "Multinomial Naive Bayes: Suitable for features representing counts or frequencies, such as word counts in text. It can perform well if your data is well-suited for this kind of representation.\n",
    "\n",
    "Gaussian Naive Bayes: Assumes Gaussian distribution for continuous features. It can be effective when your data's features follow a Gaussian distribution.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Based on the results and analysis, you can conclude which variant of Naive Bayes performs best for the given dataset. It's important to remember that the choice of the best classifier depends on the nature of your data and how well it aligns with the assumptions of each variant. Additionally, Naive Bayes classifiers might not always perform optimally, especially if the independence assumption doesn't hold well or if the data is complex. For future work, you could explore feature engineering, hyperparameter tuning, or consider more advanced classification algorithms to potentially improve performance further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
